{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d0a35eb",
   "metadata": {},
   "source": [
    "# Process a language tab to yaml v. 2.0\n",
    "\n",
    "This notebook collects language tabs from the SAPhon [Tupian Nasal Typology Input](https://docs.google.com/spreadsheets/d/1dvXFvLIV4y84CglgjAl-ZVb09IuGazs1SzFO_UJpmnI/edit#gid=1164878023) spreadsheet and creates version 2.0 yaml output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f1c03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spreadsheet\n",
    "import os, re, sys\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import json\n",
    "import math\n",
    "\n",
    "downloads = Path.home() / 'Downloads'\n",
    "langdir = Path('./newlangs/')\n",
    "langdir.mkdir(parents=True, exist_ok=True)\n",
    "yamldir = Path('../langs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f823926",
   "metadata": {},
   "source": [
    "## Get Tupian input spreadsheet lang tabs\n",
    "\n",
    "Collect the language tabs from the input spreadsheet into a dataframe, one row per lang tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ffb1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssdf = pd.DataFrame.from_records(list(spreadsheet.langsheets.values()))\n",
    "ssdf['tabname'] = list(spreadsheet.langsheets.keys())\n",
    "ssdf['yaml'] = ssdf['short'] + '.yaml'\n",
    "ssdf = ssdf[ssdf['include']].reset_index(drop=True).drop('include', axis='columns')\n",
    "assert(~ssdf['gid'].duplicated().any())\n",
    "assert(~ssdf['tabname'].duplicated().any())\n",
    "ssdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe35a452",
   "metadata": {},
   "source": [
    "## Download `.tsv` files (optional)\n",
    "\n",
    "The next cell is optional to download all lang tabs from the spreadsheet. Set `do_download` to `True` and execute the cell to do this task. For active work on a lang tab this step is not necessary and time-consuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fb2606",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_download = False\n",
    "if do_download:\n",
    "    for row in ssdf[ssdf['include']].itertuples():\n",
    "        r = requests.get(f'{spreadsheet.puburl}/pub?gid={row.gid}&single=true&output=tsv')\n",
    "        r.encoding = 'utf-8'\n",
    "        with open(langdir / f'{row.short}.tsv', 'w', encoding='utf-8') as out:\n",
    "            out.write(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd7ffc4",
   "metadata": {},
   "source": [
    "## Function definitions\n",
    "\n",
    "Functions used to create version 2.0 yaml, work in progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5436b0-a7d0-47e8-ae3a-88a1000142c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'lowering'\n",
    "procname = name if not '-' in name else name[name.index('-')+1:]\n",
    "procname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901c1421-f060-4922-9ff0-13ee9316b6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "langre = re.compile(\n",
    "    r'''\n",
    "    (?P<name>[^\\[]+)\n",
    "    (?P<iso>\\[[^\\]]+\\])?\n",
    "    ''',\n",
    "    re.VERBOSE\n",
    ")\n",
    "\n",
    "def _clean(s):\n",
    "    '''\n",
    "    Clean string of extraneous markup.\n",
    "    '''\n",
    "    if s != None:\n",
    "        s = s.strip().strip('{').strip('}').strip('[').strip(']').strip()\n",
    "    return s\n",
    "\n",
    "def alloprocs(allos, procs, phoneme):\n",
    "    '''\n",
    "    Return zipped allophones and processes extracted from allophone list.\n",
    "    '''\n",
    "    allolist = [_clean(a) for a in allos.split(',')]\n",
    "    proclist = [_clean(p) for p in procs.split(',')]\n",
    "    mapping = {a: [] for a in allolist}\n",
    "\n",
    "    try:\n",
    "        assert(len(proclist) > 0)\n",
    "    except AssertionError:\n",
    "        sys.stderr.write(f'Invalid entry for phoneme {phoneme}, must have at least one allophone and process\\n') \n",
    "    if (len(allolist) == 1):\n",
    "        mapping[allolist[0]] = proclist\n",
    "    else:\n",
    "        for proc in proclist:\n",
    "            m = spreadsheet.procre.match(proc)\n",
    "            pgd = m.groupdict()\n",
    "            try:\n",
    "                phone = pgd['phone'].replace('-', '')\n",
    "            except AttributeError:\n",
    "                sys.stderr.write(f'Process {proc} must have an allophone prefix\\n')\n",
    "                continue\n",
    "            procname = pgd['procsubtype'] if pgd['tag'] is None else pgd['tag'] + pgd['procsubtype']\n",
    "            try:\n",
    "                assert(phone in mapping.keys())\n",
    "                mapping[phone].append(procname)\n",
    "            except AssertionError:\n",
    "                sys.stderr.write(f'{proc} does not map to an allophone\\n')\n",
    "    for a, proclist in mapping.items():\n",
    "        if a == phoneme:\n",
    "            try:\n",
    "                assert(proclist == [])\n",
    "            except AssertionError:\n",
    "                sys.stderr.write(f'Identity allophone {a} should not have a process\\n')\n",
    "        else:\n",
    "            try:\n",
    "                assert(len(proclist) > 0)\n",
    "            except AssertionError:\n",
    "                sys.stderr.write(f'Non-identity allophone {a} must name at least one process\\n')\n",
    "    return [{'allophone': k, 'processnames': v} for k, v in mapping.items()]\n",
    " \n",
    "\n",
    "def tsv2newyaml(tsvfile, v1):\n",
    "    '''\n",
    "    Make a new YAML dict from a Tupian input spreadsheet tab.\n",
    "    '''\n",
    "    tsvlang = spreadsheet.read_lang(tsvfile)\n",
    "    natclasses, flatnatclasses, catsymb = spreadsheet.check_natclasses(tsvlang)\n",
    "    allophones, alloprocs = spreadsheet.check_allophones(tsvlang, flatnatclasses)\n",
    "    morph_id_map = spreadsheet.check_morpheme_ids(tsvlang)\n",
    "    spreadsheet.check_procs(tsvlang, flatnatclasses, morph_id_map, catsymb, alloprocs)\n",
    "    # TODO: remainder should be per-doc (synthesis, ref)\n",
    "    doc = tsvlang['synthesis']\n",
    "    langm = re.match(langre, doc['lang'])\n",
    "    name = langm.groupdict()['name'].strip()\n",
    "    try:\n",
    "        iso_codes = [_clean(c) for c in langm.groupdict()['iso'].split(',')]\n",
    "    except:\n",
    "        iso_codes = []\n",
    "    try:\n",
    "        notes = doc['notes']\n",
    "    except KeyError:\n",
    "        notes = 'None'\n",
    "    if v1 == {}:\n",
    "        v1 = {\n",
    "            'short_name': 'TODO',\n",
    "            'alternate_names': 'TODO',\n",
    "            'family': 'TODO',\n",
    "            'countries': 'TODO',\n",
    "            'coordinates': 'TODO',\n",
    "            'tone': 'TODO',\n",
    "            'laryngeal_harmony': 'TODO'\n",
    "        }\n",
    "    sdoc = {\n",
    "        'doctype': 'synthesis',\n",
    "        'name': _clean(langm.groupdict()['name']),\n",
    "        'glottolog_name': 'TODO',\n",
    "        'short_name': v1['short_name'],\n",
    "        'alternate_names': v1['alternate_names'],\n",
    "        'iso_codes': iso_codes,\n",
    "        'synthesis': doc['synthesis'],\n",
    "        'natural_classes': [{'symbol': nc[0], 'members': nc[1:]} for nc in natclasses['synthesis']],\n",
    "        'glottolog_codes': ['TODO'], # TODO: new, need to be added by hand\n",
    "        'family': v1['family'],\n",
    "        'countries': v1['countries'],\n",
    "        'coordinates': v1['coordinates'],\n",
    "#!        'morphemes': [], # TODO\n",
    "        'phonemes': phonlist(allophones['synthesis']),\n",
    "        'processdetails': doc['processes'],\n",
    "         # TODO: following from v1 and not mentioned in new YAML draft\n",
    "#!        'allophones': v1['allophones'],\n",
    "#!        'nasal_harmony': v1['nasal_harmony'],\n",
    "        'tone': v1['tone'],\n",
    "        'laryngeal_harmony': v1['laryngeal_harmony'],\n",
    "        'notes': notes\n",
    "    }\n",
    "    # Filter None values out of list values.\n",
    "#!    listflds = (\n",
    "#!        'alternate_names', 'iso_codes', 'glottolog_codes', \n",
    "#!        'countries', 'coordinates', 'natural_classes',\n",
    "#!        'morphemes', 'phonemes', 'processes',\n",
    "#!        'triggers', 'notes'\n",
    "#!    )\n",
    "#!    for fld in listflds:\n",
    "#!        sdoc[fld] = [v for v in sdoc[fld] if v is not None]\n",
    "    return (sdoc, tsvlang, allophones)\n",
    "\n",
    "def proclist_old(processes):\n",
    "    '''\n",
    "    Return a `processdetails` list from spreadsheet `processes` section.\n",
    "    '''\n",
    "    deets = []\n",
    "    for proc in processes:\n",
    "        procname = proc['proc_name'] if not '-' in proc['proc_name'] else proc['proc_name'][proc['proc_name'].index('-')+1:]\n",
    "        deet = {\n",
    "            'processname': procname,\n",
    "            'processtype': proc['proc_type'],\n",
    "            'description': proc['description'],\n",
    "            'optionality': proc['optionality'],\n",
    "            'directionality': proc['directionality'],\n",
    "            'alternation_type': proc['alternation_type']\n",
    "        }\n",
    "        for fld in 'undergoers', 'triggers':\n",
    "            if proc[fld] == 'NA':\n",
    "                deet[fld] = [['NA'], {'type': 'TODO', 'positional_restrictions': 'TODO'}]\n",
    "            elif isinstance(proc[fld], dict):\n",
    "                data = proc[fld][fld]\n",
    "            else:\n",
    "                print(f'PROC: {proc}')\n",
    "                data = proc[fld][0][fld]\n",
    "            try:\n",
    "                deet[fld] = [\n",
    "                    [_clean(f) for f in data.split(',')],\n",
    "                    {\n",
    "                        'type': 'TODO',\n",
    "                        'positional_restrictions': 'TODO'\n",
    "                    }\n",
    "                ]\n",
    "            except:\n",
    "                print(f'failed for {fld}: \"{proc[fld]}\"')\n",
    "        for new, old in ('transparent', 'transparencies'), ('opaque', 'opacities'):\n",
    "            deet[new] = proc[old][old]\n",
    "        deets.append(deet)\n",
    "    return deets\n",
    "\n",
    "def _clean_procname(p):\n",
    "    if p is None:\n",
    "        return 'TODO: got None'\n",
    "    else:\n",
    "        return _clean(p if '-' not in p else p[p.index('-')+1:])\n",
    "\n",
    "def phonlist(allophones):\n",
    "    '''\n",
    "    Return a `phonemes` list from an `allophone` set.\n",
    "    '''\n",
    "    phonemes = {}\n",
    "    for pset in allophones:\n",
    "        d = {}\n",
    "        if len(pset) == 5:\n",
    "            seg = pset[0]\n",
    "            d = {'phoneme': f'STRING MAPPING: \"{pset}\"'}\n",
    "        elif len(pset) == 4:\n",
    "            phoneme, allos, env, proc = pset\n",
    "            allophones = alloprocs(allos, proc, phoneme)\n",
    "            envs = []\n",
    "            for env in spreadsheet.parse_env(pset[2]):\n",
    "                env['allophones'] = allophones\n",
    "                envs.append(env)\n",
    "        elif len(pset) == 2:\n",
    "            phoneme, allos = pset\n",
    "            envs = [{\n",
    "                'preceding': '', # TODO: NA or other empty value here?\n",
    "                'following': '', # TODO: NA or other empty value here?\n",
    "                'allophones': [\n",
    "                    {\n",
    "                        'processnames': [],\n",
    "                        'allophone': _clean(a)\n",
    "                    } for a in allos.split(',')\n",
    "                ]\n",
    "            }]\n",
    "        else:\n",
    "            sys.stderr.write(f'Unexpected phoneme set length for \"{pset}\".')\n",
    "            continue\n",
    "        try:\n",
    "            phonemes[phoneme]['environments'] += envs\n",
    "        except KeyError:\n",
    "            phonemes[phoneme] = {'phoneme': phoneme, 'environments': envs}\n",
    "    # TODO: sort phonemes\n",
    "    return list(phonemes.values())\n",
    "\n",
    "def yaml2newyaml(v1):\n",
    "    '''\n",
    "    Copy a version 1 YAML dict into version 2.\n",
    "    '''\n",
    "    sdoc = {\n",
    "        'doctype': 'synthesis',\n",
    "        'name': v1['name'],\n",
    "        'glottolog_name': v1['name'], # TODO: new, check by hand\n",
    "        'short_name': v1['short_name'],\n",
    "        'alternate_names': v1['alternate_names'],\n",
    "        'iso_codes': v1['iso_codes'],\n",
    "        'glottolog_codes': [], # TODO: new, need to be added by hand\n",
    "        'family': v1['family'],\n",
    "        'countries': v1['countries'],\n",
    "        'coordinates': v1['coordinates'],\n",
    "        'natural_classes': [], # TODO: new\n",
    "        'morphemes': [], # TODO: new?\n",
    "        'phonemes': [{'phoneme': p} for p in v1['phonemes']],\n",
    "        'processdetails': [], # TODO: new?\n",
    "        'triggers': [], # TODO: new?\n",
    "        'transparent': [], # TODO: new?, include?\n",
    "        'opaque': [], # TODO: new?, include?\n",
    "         # TODO: following from v1 and not mentioned in new YAML draft\n",
    "        'allophones': v1['allophones'],\n",
    "        'nasal_harmony': v1['nasal_harmony'],\n",
    "        'tone': v1['tone'],\n",
    "        'laryngeal_harmony': v1['laryngeal_harmony'],\n",
    "        'notes': v1['notes'], # TODO: not mentioned in new YAML draft\n",
    "    }\n",
    "    # Filter None values out of list values.\n",
    "    listflds = (\n",
    "        'alternate_names', 'iso_codes', 'glottolog_codes', \n",
    "        'countries', 'coordinates', 'natural_classes',\n",
    "        'morphemes', 'phonemes', 'processdetails',\n",
    "        'triggers', 'notes'\n",
    "    )\n",
    "    for fld in listflds:\n",
    "        sdoc[fld] = [v for v in sdoc[fld] if v is not None]\n",
    "    return sdoc\n",
    "\n",
    "TODO = 'TODO'\n",
    "\n",
    "def ss2refdoc(lang):\n",
    "    pass\n",
    "\n",
    "def ss2synthdoc(lang):\n",
    "    '''\n",
    "    Produce a synthesis yaml doc from a lang from the input spreadsheet.\n",
    "    '''\n",
    "    synth = lang['synthesis']\n",
    "    langm = re.match(langre, synth['lang'])\n",
    "    yd = {\n",
    "        'doctype': 'synthesis',\n",
    "        'name': langm['name'].strip(),\n",
    "        'short_name': TODO,\n",
    "        'alternate_names': TODO,\n",
    "        'iso_codes': langm['iso']\n",
    "    }\n",
    "    return yd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a90ad26-f17e-457b-b1c6-03524e021bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "samplestrs = [\n",
    "    'p, p',\n",
    "    'e, {e, ɛ}, @, ɛ-lowering',\n",
    "    's, {s, tʃ, ʃ}, {_i, i_}, {tʃ-fortition, tʃ-palatalization, ʃ-palatalization}',\n",
    "    't, n, _+Ṽ, XMP=LN:t',\n",
    "    \"e, {eː, ɛː}, $'_, {eː-lengthening, ɛː-lowering, ɛː-lengthening}\",\n",
    "]\n",
    "for s in samplestrs:\n",
    "    pset = spreadsheet.split_outside_delims(s)\n",
    "    if len(pset) == 4:\n",
    "        print(pset)\n",
    "        phoneme, allos, env, proc = pset\n",
    "        mapping = alloprocs(allos, proc, phoneme)\n",
    "        print(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a67c36",
   "metadata": {},
   "source": [
    "## Read `.tsv` and v1 `.yaml` files\n",
    "\n",
    "Download, read, and process lang tabs (and existing v. 1 yaml files, if they exist). Set one or more tab indexes in `rng` to be checked for errors. Set `use_cached` to `True` if you want to use a previously-downloaded `.tsv` file instead of downloading from the input spreadsheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d89be0-d28b-447c-a4cf-6601ff31692b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nosynthesis = [\n",
    "    12, # no synthesis\n",
    "    25, # no synthesis\n",
    "    31, # no synthesis\n",
    "    43, # no synthesis\n",
    "    46, # no synthesis\n",
    "    47, # no synthesis\n",
    "    50, # no synthesis\n",
    "    56, # no synthesis\n",
    "#   58, 59 # temp\n",
    "]\n",
    "# investigate\n",
    "# 16 phonemes/allophones phonlist()\n",
    "# 21, 28, 34, 36, 42, 44, 51, 62, 64 proc details\n",
    "# 66 bad parse at high level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a2eb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cached = True\n",
    "langs = {}\n",
    "#rng = [0, 2, 3, 5, 20, 29, 37, 40] # Indexes that have no errors so far\n",
    "rng =  [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9]\n",
    "rng += [    11,                     17, 18, 19]  # List of one or more tab indexes to process\n",
    "rng += [20,         23, 24,         27,     29]\n",
    "rng += [30,     32, 33,                 38    ]\n",
    "rng += [40, 41,                         48, 49]\n",
    "rng += [            53,     55,         58, 59]\n",
    "rng += [60, 61,     63,               ] # 67 is out of bounds\n",
    "#rng = [58]\n",
    "rng = [e for e in rng if e not in nosynthesis]\n",
    "print(f'Working on {len(rng)} lang tabs')\n",
    "for row in ssdf.iloc[rng].itertuples():\n",
    "    if (yamldir / row.yaml).exists():\n",
    "        with open(yamldir / row.yaml, 'r', encoding='utf-8') as fh:\n",
    "            v1docs = list(yaml.safe_load_all(fh))\n",
    "            if math.isnan(v1docs[0]['coordinates'][0]['elevation_meters']):\n",
    "                v1docs[0]['coordinates'][0]['elevation_meters'] = None\n",
    "        v1synth = yaml2newyaml(v1docs[0])\n",
    "    else:\n",
    "        v1synth = {}\n",
    "    \n",
    "    tsvfile = langdir / f'{row.short}.tsv'\n",
    "    if use_cached is not True or not tsvfile.exists():\n",
    "        print(f\"Requesting '{row.tabname}' lang tab from index {row.Index} and caching at {tsvfile}.\")\n",
    "        r = requests.get(f'{spreadsheet.puburl}/pub?gid={row.gid}&single=true&output=tsv')\n",
    "        r.encoding = 'utf-8'\n",
    "        with open(tsvfile, 'w', encoding='utf-8') as out:\n",
    "            # Replace Windows CRLF with Unix LF\n",
    "            text = r.content.replace(b'\\r\\n', b'\\n').decode('utf8')\n",
    "            out.write(text)\n",
    "    else:\n",
    "        print(f\"Reading from cached file {tsvfile}.\")\n",
    "    try:\n",
    "        v2synth, tsvlang, allophones = tsv2newyaml(tsvfile, v1synth)\n",
    "    except Exception as e:\n",
    "        v2synth = {}\n",
    "        sys.stderr.write(f'ERROR: spreadsheet tab {row.tabname} failed.\\n{e}')\n",
    "        raise e\n",
    "    langs[row.short] = {\n",
    "        'v1synth': v1synth,\n",
    "        'v2synth': v2synth,\n",
    "        'tsv': tsvlang['synthesis']\n",
    "    }\n",
    "    try:\n",
    "        with open(langdir / 'json' / f'{row.short}.json', 'w', encoding='utf8') as out:\n",
    "            json.dump(langs[row.short]['v2synth'], out, indent=2, ensure_ascii=False)\n",
    "        print(f'Dumped json {row.short}.json')\n",
    "    except Exception as e:\n",
    "        sys.stderr.write(f'Failed to dump json {row.short}.json.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4447c9b-850a-4a69-80bb-70e229e569a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1docs[0]['coordinates'][0]['elevation_meters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2647d03-0a59-4a6b-99ac-ea900b7aa06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bytes(f'{spreadsheet.normalizeIPA(\"ĩ\")} {spreadsheet.normalizeIPA(\"ĩ\")} ĩ', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c812581d-1baa-471d-8148-f6539b0d3f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dumps?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4092f9d6-cb8d-4a06-9319-add1476b018d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spreadsheet.allowed_phon"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
