{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d0a35eb",
   "metadata": {},
   "source": [
    "# Process a language tab to yaml v. 2.0\n",
    "\n",
    "This notebook collects language tabs from the SAPhon [Tupian Nasal Typology Input](https://docs.google.com/spreadsheets/d/1dvXFvLIV4y84CglgjAl-ZVb09IuGazs1SzFO_UJpmnI/edit#gid=1164878023) spreadsheet and creates version 2.0 yaml output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f1c03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spreadsheet\n",
    "import os, re, sys\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import json\n",
    "\n",
    "downloads = Path.home() / 'Downloads'\n",
    "langdir = Path('./newlangs/')\n",
    "langdir.mkdir(parents=True, exist_ok=True)\n",
    "yamldir = Path('../langs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f823926",
   "metadata": {},
   "source": [
    "## Get Tupian input spreadsheet lang tabs\n",
    "\n",
    "Collect the language tabs from the input spreadsheet into a dataframe, one row per lang tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ffb1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssdf = pd.DataFrame.from_records(list(spreadsheet.langsheets.values()))\n",
    "ssdf['tabname'] = list(spreadsheet.langsheets.keys())\n",
    "ssdf['yaml'] = ssdf['short'] + '.yaml'\n",
    "ssdf = ssdf[ssdf['include']].reset_index(drop=True).drop('include', axis='columns')\n",
    "assert(~ssdf['gid'].duplicated().any())\n",
    "assert(~ssdf['tabname'].duplicated().any())\n",
    "ssdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe35a452",
   "metadata": {},
   "source": [
    "## Download `.tsv` files (optional)\n",
    "\n",
    "The next cell is optional to download all lang tabs from the spreadsheet. Set `do_download` to `True` and execute the cell to do this task. For active work on a lang tab this step is not necessary and time-consuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fb2606",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_download = False\n",
    "if do_download:\n",
    "    for row in ssdf[ssdf['include']].itertuples():\n",
    "        r = requests.get(f'{spreadsheet.puburl}/pub?gid={row.gid}&single=true&output=tsv')\n",
    "        r.encoding = 'utf-8'\n",
    "        with open(langdir / f'{row.short}.tsv', 'w', encoding='utf-8') as out:\n",
    "            out.write(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd7ffc4",
   "metadata": {},
   "source": [
    "## Function definitions\n",
    "\n",
    "Functions used to create version 2.0 yaml, work in progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5436b0-a7d0-47e8-ae3a-88a1000142c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'lowering'\n",
    "procname = name if not '-' in name else name[name.index('-')+1:]\n",
    "procname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f94af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "langre = re.compile(\n",
    "    r'''\n",
    "    (?P<name>[^\\[]+)\n",
    "    (?P<iso>\\[[^\\]]+\\])?\n",
    "    ''',\n",
    "    re.VERBOSE\n",
    ")\n",
    "\n",
    "def _clean(s):\n",
    "    '''\n",
    "    Clean string of extraneous markup.\n",
    "    '''\n",
    "    if s != None:\n",
    "        s = s.strip().strip('{').strip('}').strip('[').strip(']').strip()\n",
    "    return s\n",
    "\n",
    "def alloprocs(allos, procs, phoneme):\n",
    "    '''\n",
    "    Return zipped allophones and processes extracted from allophone list.\n",
    "    '''\n",
    "    allolist = [_clean(a) for a in allos.split(',')]\n",
    "    proclist = [_clean(p) for p in procs.split(',')]\n",
    "    if len(proclist) != len(allolist):\n",
    "        try:\n",
    "            proclist.insert(allolist.index(phoneme), None)\n",
    "        except ValueError:\n",
    "            sys.stderr.write(f'Cannot find identity phone {phoneme} in allophone list {allolist} for proc list {proclist}\\n')\n",
    "        for allo, proc in zip(allolist, proclist):\n",
    "            if proc is None:\n",
    "                continue\n",
    "            m = re.match(spreadsheet.procre, proc)\n",
    "            try:\n",
    "                phone = m.group('phone').strip('-')\n",
    "            except Exception as e:\n",
    "                sys.stderr.write(f\"Problem parsing proc '{proc}': {e}\\nGot {m.groupdict()}\")\n",
    "                continue\n",
    "            try:\n",
    "                assert(phone == allo)\n",
    "            except AssertionError:\n",
    "                sys.stderr.write(f'Proc phone {phone} does not match allophone {allo}\\n')\n",
    "    return zip(allolist, proclist)\n",
    "\n",
    "def tsv2newyaml(tsvfile):\n",
    "    '''\n",
    "    Make a new YAML dict from a Tupian input spreadsheet tab.\n",
    "    '''\n",
    "    tsvlang = spreadsheet.read_lang(tsvfile)\n",
    "    natclasses, flatnatclasses, catsymb = spreadsheet.check_natclasses(tsvlang)\n",
    "    allophones, alloprocs = spreadsheet.check_allophones(tsvlang, flatnatclasses)\n",
    "    morph_id_map = spreadsheet.check_morpheme_ids(tsvlang)\n",
    "    spreadsheet.check_procs(tsvlang, flatnatclasses, morph_id_map, catsymb, alloprocs)\n",
    "    # TODO: remainder should be per-doc (synthesis, ref)\n",
    "    doc = tsvlang['synthesis']\n",
    "    langm = re.match(langre, doc['lang'])\n",
    "    name = langm.groupdict()['name'].strip()\n",
    "    try:\n",
    "        iso_codes = [_clean(c) for c in langm.groupdict()['iso'].split(',')]\n",
    "    except:\n",
    "        iso_codes = []\n",
    "    try:\n",
    "        notes = doc['notes']\n",
    "    except KeyError:\n",
    "        notes = 'None'\n",
    "    sdoc = {\n",
    "        'doctype': 'synthesis',\n",
    "        'name': _clean(langm.groupdict()['name']),\n",
    "#        'glottolog_name': v1['name'], # TODO: new, check by hand\n",
    "#        'short_name': v1['short_name'],\n",
    "#        'alternate_names': v1['alternate_names'],\n",
    "        'iso_codes': iso_codes,\n",
    "        'synthesis': doc['synthesis'],\n",
    "        'natural_classes': [{'symbol': nc[0], 'members': nc[1:]} for nc in natclasses['synthesis']],\n",
    "#        'glottolog_codes': [], # TODO: new, need to be added by hand\n",
    "#        'family': v1['family'],\n",
    "#        'countries': v1['countries'],\n",
    "#        'coordinates': v1['coordinates'],\n",
    "#!        'natural_classes': [], # TODO: new\n",
    "#!        'morphemes': [], # TODO: new?\n",
    "        'phonemes': phonlist(allophones['synthesis']),\n",
    "        'processes': proclist(doc['processes']), # TODO: new?\n",
    "#!        'triggers': [], # TODO: new?\n",
    "#!        'transparent': [], # TODO: new?, include?\n",
    "#!        'opaque': [], # TODO: new?, include?\n",
    "         # TODO: following from v1 and not mentioned in new YAML draft\n",
    "#!        'allophones': v1['allophones'],\n",
    "#!        'nasal_harmony': v1['nasal_harmony'],\n",
    "#!        'tone': v1['tone'],\n",
    "#!        'laryngeal_harmony': v1['laryngeal_harmony'],\n",
    "        'notes': notes\n",
    "    }\n",
    "    # Filter None values out of list values.\n",
    "#!    listflds = (\n",
    "#!        'alternate_names', 'iso_codes', 'glottolog_codes', \n",
    "#!        'countries', 'coordinates', 'natural_classes',\n",
    "#!        'morphemes', 'phonemes', 'processes',\n",
    "#!        'triggers', 'notes'\n",
    "#!    )\n",
    "#!    for fld in listflds:\n",
    "#!        sdoc[fld] = [v for v in sdoc[fld] if v is not None]\n",
    "    return (sdoc, tsvlang, allophones)\n",
    "\n",
    "def proclist(processes):\n",
    "    '''\n",
    "    Return a `processdetails` list from spreadsheet `processes` section.\n",
    "    '''\n",
    "    deets = []\n",
    "    for proc in processes:\n",
    "        procname = proc['proc_name'] if not '-' in proc['proc_name'] else proc['proc_name'][proc['proc_name'].index('-')+1:]\n",
    "        deet = {\n",
    "            'processname': procname,\n",
    "            'processtype': proc['proc_type'],\n",
    "            'description': proc['description'],\n",
    "            'optionality': proc['optionality'],\n",
    "            'directionality': proc['directionality'],\n",
    "            'alternation_type': proc['alternation_type']\n",
    "        }\n",
    "        for fld in 'undergoers', 'triggers':\n",
    "            if proc[fld] == 'NA':\n",
    "                deet[fld] = [['NA'], {'type': 'TODO', 'positional_restrictions': 'TODO'}]\n",
    "            elif isinstance(proc[fld], dict):\n",
    "                data = proc[fld][fld]\n",
    "            else:\n",
    "                data = proc[fld][0][fld]\n",
    "            try:\n",
    "                deet[fld] = [\n",
    "                    [_clean(f) for f in data.split(',')],\n",
    "                    {\n",
    "                        'type': 'TODO',\n",
    "                        'positional_restrictions': 'TODO'\n",
    "                    }\n",
    "                ]\n",
    "            except:\n",
    "                print(f'failed for {fld}: \"{proc[fld]}\"')\n",
    "        for new, old in ('transparent', 'transparencies'), ('opaque', 'opacities'):\n",
    "            deet[new] = [[proc['transparencies'], {\n",
    "                'type': 'segmental',\n",
    "                'positional_restrictions': 'None'\n",
    "            }]]\n",
    "        deets.append(deet)\n",
    "    return deets\n",
    "\n",
    "def _clean_procname(p):\n",
    "    if p is None:\n",
    "        return 'TODO: got None'\n",
    "    else:\n",
    "        return _clean(p if '-' not in p else p[p.index('-')+1:])\n",
    "\n",
    "def phonlist(allophones):\n",
    "    '''\n",
    "    Return a `phonemes` list from an `allophone` set.\n",
    "    '''\n",
    "    phonemes = {}\n",
    "    for pset in allophones:\n",
    "        d = {}\n",
    "        if len(pset) == 5:\n",
    "            seg = pset[0]\n",
    "            d = {'phoneme': f'STRING MAPPING: \"{pset}\"'}\n",
    "        elif len(pset) == 4:\n",
    "            phoneme, allos, env, proc = pset\n",
    "            envs = []\n",
    "            for env in pset[2].split(','):\n",
    "                if env == '@':\n",
    "                    prec, foll = 'TODO:@', 'TODO:@' # TODO: check meaning of '@'\n",
    "                else:\n",
    "                    try:\n",
    "                        prec, foll = env.split('_')\n",
    "                    except:\n",
    "                        # TODO: resolve how to handle this error\n",
    "                        prec, foll = f'ERROR: env value: \"{env}\"', ''\n",
    "                envs.append({\n",
    "                    'preceding': prec,\n",
    "                    'following': foll,\n",
    "                    'processes': [\n",
    "                        {\n",
    "                            'processnames': [_clean_procname(p)],\n",
    "                            'allophone': _clean(a)\n",
    "                        } for a, p in alloprocs(allos, proc, phoneme)\n",
    "                    ]\n",
    "                })\n",
    "        elif len(pset) == 2:\n",
    "            phoneme, allos = pset\n",
    "            envs = [{\n",
    "                'preceding': '', # TODO: NA or other empty value here?\n",
    "                'following': '', # TODO: NA or other empty value here?\n",
    "                'processes': [\n",
    "                    {\n",
    "                        'processnames': [],\n",
    "                        'allophone': _clean(a)\n",
    "                    } for a in allos.split(',')\n",
    "                ]\n",
    "            }]\n",
    "        else:\n",
    "            sys.stderr.write(f'Unexpected phoneme set length for \"{pset}\".')\n",
    "            continue\n",
    "        try:\n",
    "            phonemes[phoneme]['environments'] += envs\n",
    "        except KeyError:\n",
    "            phonemes[phoneme] = {'phoneme': phoneme, 'environments': envs}\n",
    "    # TODO: sort phonemes\n",
    "    return list(phonemes.values())\n",
    "\n",
    "def yaml2newyaml(v1):\n",
    "    '''\n",
    "    Copy a version 1 YAML dict into version 2.\n",
    "    '''\n",
    "    sdoc = {\n",
    "        'doctype': 'synthesis',\n",
    "        'name': v1['name'],\n",
    "        'glottolog_name': v1['name'], # TODO: new, check by hand\n",
    "        'short_name': v1['short_name'],\n",
    "        'alternate_names': v1['alternate_names'],\n",
    "        'iso_codes': v1['iso_codes'],\n",
    "        'glottolog_codes': [], # TODO: new, need to be added by hand\n",
    "        'family': v1['family'],\n",
    "        'countries': v1['countries'],\n",
    "        'coordinates': v1['coordinates'],\n",
    "        'natural_classes': [], # TODO: new\n",
    "        'morphemes': [], # TODO: new?\n",
    "        'phonemes': [{'phoneme': p} for p in v1['phonemes']],\n",
    "        'processdetails': [], # TODO: new?\n",
    "        'triggers': [], # TODO: new?\n",
    "        'transparent': [], # TODO: new?, include?\n",
    "        'opaque': [], # TODO: new?, include?\n",
    "         # TODO: following from v1 and not mentioned in new YAML draft\n",
    "        'allophones': v1['allophones'],\n",
    "        'nasal_harmony': v1['nasal_harmony'],\n",
    "        'tone': v1['tone'],\n",
    "        'laryngeal_harmony': v1['laryngeal_harmony'],\n",
    "        'notes': v1['notes'], # TODO: not mentioned in new YAML draft\n",
    "    }\n",
    "    # Filter None values out of list values.\n",
    "    listflds = (\n",
    "        'alternate_names', 'iso_codes', 'glottolog_codes', \n",
    "        'countries', 'coordinates', 'natural_classes',\n",
    "        'morphemes', 'phonemes', 'processdetails',\n",
    "        'triggers', 'notes'\n",
    "    )\n",
    "    for fld in listflds:\n",
    "        sdoc[fld] = [v for v in sdoc[fld] if v is not None]\n",
    "    return sdoc\n",
    "\n",
    "TODO = 'TODO'\n",
    "\n",
    "def ss2refdoc(lang):\n",
    "    pass\n",
    "\n",
    "def ss2synthdoc(lang):\n",
    "    '''\n",
    "    Produce a synthesis yaml doc from a lang from the input spreadsheet.\n",
    "    '''\n",
    "    synth = lang['synthesis']\n",
    "    langm = re.match(langre, synth['lang'])\n",
    "    yd = {\n",
    "        'doctype': 'synthesis',\n",
    "        'name': langm['name'].strip(),\n",
    "        'short_name': TODO,\n",
    "        'alternate_names': TODO,\n",
    "        'iso_codes': langm['iso']\n",
    "    }\n",
    "    return yd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a67c36",
   "metadata": {},
   "source": [
    "## Read `.tsv` and v1 `.yaml` files\n",
    "\n",
    "Download, read, and process lang tabs (and existing v. 1 yaml files, if they exist). Set one or more tab indexes in `rng` to be checked for errors. Set `use_cached` to `True` if you want to use a previously-downloaded `.tsv` file instead of downloading from the input spreadsheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a2eb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cached = True\n",
    "langs = {}\n",
    "#rng = [0, 2, 3, 5, 20, 29, 37, 40] # Indexes that have no errors so far\n",
    "rng = [22]  # List of one or more tab indexes to process\n",
    "for row in ssdf.iloc[rng].itertuples():\n",
    "    if (yamldir / row.yaml).exists():\n",
    "        with open(yamldir / row.yaml, 'r', encoding='utf-8') as fh:\n",
    "            v1docs = list(yaml.safe_load_all(fh))\n",
    "        v1synth = yaml2newyaml(v1docs[0])\n",
    "    else:\n",
    "        v1synth = {}\n",
    "    \n",
    "    tsvfile = langdir / f'{row.short}.tsv'\n",
    "    if use_cached is not True or not tsvfile.exists():\n",
    "        print(f\"Requesting '{row.tabname}' lang tab from index {row.Index} and caching at {tsvfile}\")\n",
    "        r = requests.get(f'{spreadsheet.puburl}/pub?gid={row.gid}&single=true&output=tsv')\n",
    "        r.encoding = 'utf-8'\n",
    "        with open(tsvfile, 'w', encoding='utf-8') as out:\n",
    "            # Replace Windows CRLF with Unix LF\n",
    "            text = r.content.replace(b'\\r\\n', b'\\n').decode('utf8')\n",
    "            out.write(text)\n",
    "    try:\n",
    "        v2synth, tsvlang, allophones = tsv2newyaml(tsvfile)\n",
    "    except Exception as e:\n",
    "        v2synth = {}\n",
    "        sys.stderr.write(f'ERROR: spreadsheet tab {row.tabname} failed.\\n{e}')\n",
    "        raise e\n",
    "    langs[row.short] = {\n",
    "        'v1synth': v1synth,\n",
    "        'v2synth': v2synth,\n",
    "        'tsv': tsvlang['synthesis']\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fc261e-f6bf-4ff3-94c3-cc05b804957d",
   "metadata": {},
   "source": [
    "## Sample json dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed835ca8-537c-437f-a65c-0928d64918a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = 'Nhandeva'\n",
    "with open(f'{lang}.json', 'w', encoding='utf8') as out:\n",
    "    json.dump(langs[lang]['v2synth'], out, indent=2, ensure_ascii=False)\n",
    "!cat Nhandeva.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
