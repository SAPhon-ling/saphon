{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d0a35eb",
   "metadata": {},
   "source": [
    "# Process a language tab to yaml v. 2.0\n",
    "\n",
    "This notebook collects language tabs from the SAPhon [Tupian Nasal Typology Input](https://docs.google.com/spreadsheets/d/1dvXFvLIV4y84CglgjAl-ZVb09IuGazs1SzFO_UJpmnI/edit#gid=1164878023) spreadsheet and creates version 2.0 yaml output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f1c03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spreadsheet\n",
    "import os, re, sys\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import json\n",
    "import math\n",
    "\n",
    "downloads = Path.home() / 'Downloads'\n",
    "langdir = Path('./newlangs/')\n",
    "langdir.mkdir(parents=True, exist_ok=True)\n",
    "yamldir = Path('../langs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f823926",
   "metadata": {},
   "source": [
    "## Get Tupian input spreadsheet lang tabs\n",
    "\n",
    "Collect the language tabs from the input spreadsheet into a dataframe, one row per lang tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ffb1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssdf = pd.DataFrame.from_records(list(spreadsheet.langsheets.values()))\n",
    "ssdf['tabname'] = list(spreadsheet.langsheets.keys())\n",
    "ssdf['yaml'] = ssdf['short'] + '.yaml'\n",
    "ssdf = ssdf[ssdf['include']].reset_index(drop=True).drop('include', axis='columns')\n",
    "assert(~ssdf['gid'].duplicated().any())\n",
    "assert(~ssdf['tabname'].duplicated().any())\n",
    "ssdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe35a452",
   "metadata": {},
   "source": [
    "## Download `.tsv` files (optional)\n",
    "\n",
    "The next cell is optional to download all lang tabs from the spreadsheet. Set `do_download` to `True` and execute the cell to do this task. For active work on a lang tab this step is not necessary and time-consuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fb2606",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_download = False\n",
    "if do_download:\n",
    "    for row in ssdf[ssdf['include']].itertuples():\n",
    "        r = requests.get(f'{spreadsheet.puburl}/pub?gid={row.gid}&single=true&output=tsv')\n",
    "        r.encoding = 'utf-8'\n",
    "        with open(langdir / f'{row.short}.tsv', 'w', encoding='utf-8') as out:\n",
    "            out.write(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd7ffc4",
   "metadata": {},
   "source": [
    "## Function definitions\n",
    "\n",
    "Functions used to create version 2.0 yaml, work in progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901c1421-f060-4922-9ff0-13ee9316b6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#langre = re.compile(\n",
    "#    r'''\n",
    "#    (?P<name>[^\\[]+)\n",
    "#    (?P<iso>\\[[^\\]]+\\])?\n",
    "#    ''',\n",
    "#    re.VERBOSE\n",
    "#)\n",
    "\n",
    "def _clean(s):\n",
    "    '''\n",
    "    Clean string of extraneous markup.\n",
    "    '''\n",
    "    if s != None:\n",
    "        s = s.strip().strip('{').strip('}').strip('[').strip(']').strip()\n",
    "    return s\n",
    "\n",
    "def split_morph_ids(mids):\n",
    "    morphs = []\n",
    "    if mids is None or mids == '' or mids.lower().strip() == 'none':\n",
    "        return morphs\n",
    "    for mid in spreadsheet.parse_with_delims(mids):\n",
    "        try:\n",
    "            d = {\n",
    "                'morpheme_id': mid[0],\n",
    "                'morpheme_type': mid[1],\n",
    "                'underlying_form': mid[2],\n",
    "                'surface_forms': spreadsheet.parse_with_delims(mid[3].strip('{').strip('}')),\n",
    "                'gloss': mid[4]\n",
    "            }\n",
    "        except Exception as e:\n",
    "            msg = f'Could not parse morpheme ids: {mid}: {e}'\n",
    "            raise RuntimeError(msg)\n",
    "        morphs.append(d)\n",
    "    return morphs\n",
    "\n",
    "def alloprocs(allos, procs, phoneme):\n",
    "    '''\n",
    "    Return zipped allophones and processes extracted from allophone list.\n",
    "    '''\n",
    "    allolist = [_clean(a) for a in allos.split(',')]\n",
    "    proclist = [_clean(p) for p in procs.split(',')]\n",
    "    mapping = {a: [] for a in allolist}\n",
    "\n",
    "    try:\n",
    "        assert(len(proclist) > 0)\n",
    "    except AssertionError:\n",
    "        sys.stderr.write(f'Invalid entry for phoneme {phoneme}, must have at least one allophone and process\\n') \n",
    "    if (len(allolist) == 1):\n",
    "        mapping[allolist[0]] = proclist\n",
    "    else:\n",
    "        for proc in proclist:\n",
    "            m = spreadsheet.procre.match(proc)\n",
    "            pgd = m.groupdict()\n",
    "            try:\n",
    "                phone = pgd['phone'].replace('-', '')\n",
    "            except AttributeError:\n",
    "                sys.stderr.write(f'Process {proc} must have an allophone prefix\\n')\n",
    "                continue\n",
    "            procname = pgd['procsubtype'] if pgd['tag'] is None else pgd['tag'] + pgd['procsubtype']\n",
    "            try:\n",
    "                assert(phone in mapping.keys())\n",
    "                mapping[phone].append(procname)\n",
    "            except AssertionError:\n",
    "                sys.stderr.write(f'{proc} does not map to an allophone\\n')\n",
    "    for a, proclist in mapping.items():\n",
    "        if a == phoneme:\n",
    "            try:\n",
    "                assert(proclist == [])\n",
    "            except AssertionError:\n",
    "                if proclist == ['preservation']:\n",
    "                    pass\n",
    "                else:\n",
    "                    sys.stderr.write(f'Identity allophone {a} should not have a process\\n')\n",
    "        else:\n",
    "            try:\n",
    "                assert(len(proclist) > 0)\n",
    "            except AssertionError:\n",
    "                sys.stderr.write(f'Non-identity allophone {a} must name at least one process\\n')\n",
    "    return [{'allophone': k, 'processnames': v} for k, v in mapping.items()]\n",
    " \n",
    "\n",
    "def tsv2newyaml(tsvfile, v1):\n",
    "    '''\n",
    "    Make a new YAML dict from a Tupian input spreadsheet tab.\n",
    "    '''\n",
    "    tsvlang = spreadsheet.read_lang(tsvfile)\n",
    "    natclasses, flatnatclasses, catsymb = spreadsheet.check_natclasses(tsvlang)\n",
    "    allophones, alloprocs = spreadsheet.check_allophones(tsvlang, flatnatclasses)\n",
    "    morph_id_map = spreadsheet.check_morpheme_ids(tsvlang)\n",
    "    spreadsheet.check_procs(tsvlang, flatnatclasses, morph_id_map, catsymb, alloprocs)\n",
    "    # TODO: remainder should be per-doc (synthesis, ref)\n",
    "    doc = tsvlang['synthesis']\n",
    "    name = _clean(doc['lang'])\n",
    "    if '[' in name:\n",
    "        sys.stderr.write(f'Language name for {name} should be cleaned up.\\n')\n",
    "    try:\n",
    "        notes = doc['notes']\n",
    "    except KeyError:\n",
    "        notes = 'None'\n",
    "    try:\n",
    "        glottocode = _clean(doc['glottocode'])\n",
    "    except KeyError:\n",
    "        sys.stderr.write(f'Lang {name} missing glottocode field.\\n')\n",
    "        glottocode = TODO\n",
    "    try:\n",
    "        altnames = [_clean(c) for c in doc['altnames'].split(',')]\n",
    "    except KeyError:\n",
    "        sys.stderr.write(f'Lang {name} missing alternate_names field.\\n')\n",
    "        altnames = [TODO]\n",
    "    try:\n",
    "        iso_codes = [_clean(c) for c in doc['iso_codes'].split(',')]\n",
    "    except KeyError:\n",
    "        sys.stderr.write(f'Lang {name} missing iso_codes field.\\n')\n",
    "        iso_codes = [TODO]\n",
    "    if v1 == {}:\n",
    "        v1 = {\n",
    "            'short_name': 'TODO',\n",
    "            'alternate_names': 'TODO',\n",
    "            'family': 'TODO',\n",
    "            'countries': 'TODO',\n",
    "            'coordinates': 'TODO',\n",
    "            'tone': 'TODO',\n",
    "            'laryngeal_harmony': 'TODO'\n",
    "        }\n",
    "    try:\n",
    "        sdoc = {\n",
    "            'doctype': 'synthesis',\n",
    "            'name': name,\n",
    "            'glottolog_code': glottocode,\n",
    "            'short_name': v1['short_name'],\n",
    "            'alternate_names': altnames,\n",
    "            'iso_codes': iso_codes,\n",
    "            'synthesis': doc['synthesis'],\n",
    "            'natural_classes': [{'symbol': nc[0], 'members': nc[1:]} for nc in natclasses['synthesis']],\n",
    "            'glottolog_codes': ['TODO'], # TODO: new, need to be added by hand\n",
    "            'family': v1['family'],\n",
    "            'countries': v1['countries'],\n",
    "            'coordinates': v1['coordinates'],\n",
    "            'morphemes': split_morph_ids(doc['morph_ids']),\n",
    "            'phonemes': phonlist(allophones['synthesis']),\n",
    "            'processdetails': doc['processes'],\n",
    "         # TODO: following from v1 and not mentioned in new YAML draft\n",
    "#!        'allophones': v1['allophones'],\n",
    "#!        'nasal_harmony': v1['nasal_harmony'],\n",
    "            'tone': v1['tone'],\n",
    "            'laryngeal_harmony': v1['laryngeal_harmony'],\n",
    "            'notes': notes\n",
    "        }\n",
    "    except Exception as e:\n",
    "        sys.stderr.write(f'Problem with doc {doc}.\\n')\n",
    "        raise e\n",
    "    # Filter None values out of list values.\n",
    "#!    listflds = (\n",
    "#!        'alternate_names', 'iso_codes', 'glottolog_codes', \n",
    "#!        'countries', 'coordinates', 'natural_classes',\n",
    "#!        'morphemes', 'phonemes', 'processes',\n",
    "#!        'triggers', 'notes'\n",
    "#!    )\n",
    "#!    for fld in listflds:\n",
    "#!        sdoc[fld] = [v for v in sdoc[fld] if v is not None]\n",
    "    return (sdoc, tsvlang, allophones)\n",
    "\n",
    "def proclist_old(processes):\n",
    "    '''\n",
    "    Return a `processdetails` list from spreadsheet `processes` section.\n",
    "    '''\n",
    "    deets = []\n",
    "    for proc in processes:\n",
    "        procname = proc['proc_name'] if not '-' in proc['proc_name'] else proc['proc_name'][proc['proc_name'].index('-')+1:]\n",
    "        deet = {\n",
    "            'processname': procname,\n",
    "            'processtype': proc['proc_type'],\n",
    "            'description': proc['description'],\n",
    "            'optionality': proc['optionality'],\n",
    "            'directionality': proc['directionality'],\n",
    "            'alternation_type': proc['alternation_type']\n",
    "        }\n",
    "        for fld in 'undergoers', 'triggers':\n",
    "            if proc[fld] == 'NA':\n",
    "                deet[fld] = [['NA'], {'type': 'TODO', 'positional_restrictions': 'TODO'}]\n",
    "            elif isinstance(proc[fld], dict):\n",
    "                data = proc[fld][fld]\n",
    "            else:\n",
    "                print(f'PROC: {proc}')\n",
    "                data = proc[fld][0][fld]\n",
    "            try:\n",
    "                deet[fld] = [\n",
    "                    [_clean(f) for f in data.split(',')],\n",
    "                    {\n",
    "                        'type': 'TODO',\n",
    "                        'positional_restrictions': 'TODO'\n",
    "                    }\n",
    "                ]\n",
    "            except:\n",
    "                print(f'failed for {fld}: \"{proc[fld]}\"')\n",
    "        for new, old in ('transparent', 'transparencies'), ('opaque', 'opacities'):\n",
    "            deet[new] = proc[old][old]\n",
    "        deets.append(deet)\n",
    "    return deets\n",
    "\n",
    "def _clean_procname(p):\n",
    "    if p is None:\n",
    "        return 'TODO: got None'\n",
    "    else:\n",
    "        return _clean(p if '-' not in p else p[p.index('-')+1:])\n",
    "\n",
    "def phonlist(allophones):\n",
    "    '''\n",
    "    Return a `phonemes` list from an `allophone` set.\n",
    "    '''\n",
    "    phonemes = {}\n",
    "    for pset in allophones:\n",
    "        d = {}\n",
    "        if len(pset) == 5:\n",
    "            seg = pset[0]\n",
    "            envs = {'phoneme': f'STRING MAPPING: \"{pset}\"'}\n",
    "            print(f'STRING MAPPING: \"{pset}\"')\n",
    "        elif len(pset) == 4:\n",
    "            phoneme, allos, _psetenv, proc = pset\n",
    "            allophones = alloprocs(allos, proc, phoneme)\n",
    "            envs = []\n",
    "            for env in spreadsheet.parse_env(pset[2]):\n",
    "                try:\n",
    "                    env['allophones'] = allophones\n",
    "                except:\n",
    "                    print(env)\n",
    "                envs.append(env)\n",
    "        elif len(pset) == 2:\n",
    "            phoneme, allos = pset\n",
    "            envs = [{\n",
    "                'preceding': '', # TODO: NA or other empty value here?\n",
    "                'following': '', # TODO: NA or other empty value here?\n",
    "                'allophones': [\n",
    "                    {\n",
    "                        'processnames': [],\n",
    "                        'allophone': _clean(a)\n",
    "                    } for a in allos.split(',')\n",
    "                ]\n",
    "            }]\n",
    "        else:\n",
    "            sys.stderr.write(f'Unexpected phoneme set length for \"{pset}\".')\n",
    "            continue\n",
    "        try:\n",
    "            phonemes[phoneme]['environments'] += envs\n",
    "        except KeyError:\n",
    "            phonemes[phoneme] = {'phoneme': phoneme, 'environments': envs}\n",
    "    # TODO: sort phonemes\n",
    "    return list(phonemes.values())\n",
    "\n",
    "def yaml2newyaml(v1):\n",
    "    '''\n",
    "    Copy a version 1 YAML dict into version 2.\n",
    "    '''\n",
    "    sdoc = {\n",
    "        'doctype': 'synthesis',\n",
    "        'name': v1['name'],\n",
    "        'glottolog_name': v1['name'], # TODO: new, check by hand\n",
    "        'short_name': v1['short_name'],\n",
    "        'alternate_names': v1['alternate_names'],\n",
    "        'iso_codes': v1['iso_codes'],\n",
    "        'glottolog_codes': [], # TODO: new, need to be added by hand\n",
    "        'family': v1['family'],\n",
    "        'countries': v1['countries'],\n",
    "        'coordinates': v1['coordinates'],\n",
    "        'natural_classes': [], # TODO: new\n",
    "        'morphemes': [], # TODO: new?\n",
    "        'phonemes': [{'phoneme': p} for p in v1['phonemes']],\n",
    "        'processdetails': [], # TODO: new?\n",
    "        'triggers': [], # TODO: new?\n",
    "        'transparent': [], # TODO: new?, include?\n",
    "        'opaque': [], # TODO: new?, include?\n",
    "         # TODO: following from v1 and not mentioned in new YAML draft\n",
    "        'allophones': v1['allophones'],\n",
    "        'nasal_harmony': v1['nasal_harmony'],\n",
    "        'tone': v1['tone'],\n",
    "        'laryngeal_harmony': v1['laryngeal_harmony'],\n",
    "        'notes': v1['notes'], # TODO: not mentioned in new YAML draft\n",
    "    }\n",
    "    # Filter None values out of list values.\n",
    "    listflds = (\n",
    "        'alternate_names', 'iso_codes', 'glottolog_codes', \n",
    "        'countries', 'coordinates', 'natural_classes',\n",
    "        'morphemes', 'phonemes', 'processdetails',\n",
    "        'triggers', 'notes'\n",
    "    )\n",
    "    for fld in listflds:\n",
    "        sdoc[fld] = [v for v in sdoc[fld] if v is not None]\n",
    "    return sdoc\n",
    "\n",
    "TODO = 'TODO'\n",
    "\n",
    "def ss2refdoc(lang):\n",
    "    pass\n",
    "\n",
    "def ss2synthdoc(lang):\n",
    "    '''\n",
    "    Produce a synthesis yaml doc from a lang from the input spreadsheet.\n",
    "    '''\n",
    "    synth = lang['synthesis']\n",
    "    langm = re.match(langre, synth['lang'])\n",
    "    yd = {\n",
    "        'doctype': 'synthesis',\n",
    "        'name': langm['name'].strip(),\n",
    "        'short_name': TODO,\n",
    "        'alternate_names': TODO,\n",
    "        'iso_codes': langm['iso']\n",
    "    }\n",
    "    return yd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5391e535-ffd1-4c3f-8a7a-d51925623a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just a debugging cell\n",
    "\n",
    "sample = '''{pá, stem, pá, má, US}, {potá, stem, potá, motá, US}, {ɣʷá, postposition, ɣʷá, ŋʷá, US}, {ɣʷaré, postposition, ɣʷaré, ŋʷaré, US}, {tɨ́, suffix, tɨ́, nɨ́, US}, {pɨ́, suffix, pɨ́, mɨ́, US}, {kʷéra, stem, kʷéra, ŋʷéra, US}, {ɣʷivé, stem, ɣʷivé, ŋʷivé, US}, {pukú, stem, pukú, mukú, US}, {ɣʷasú, stem, ɣʷasú, ŋʷasú, US}, {kʷé, der.affix, kʷé, ŋʷé, US}, {pe, postposition, pe, {mẽ, ʋe, ʋ̃ẽ}, US}, {ʔó, der.affix, ʔó, ʔṍ, US}, {sé, der.affix, sé, sẽ́, US}, {ɣʷi, postposition, ɣʷi, ɣ̃ʷ̃ĩ, conjunction}, {ke, stem, ke, kẽ, modifier}, {ko, stem, ko, kõ, pronoun}, {ku, stem, ku, kũ, article}, {la, stem, la, l̃ã, article}, {na...i, stem, na...i, nã...ĩ, modifier}, {pa, stem, pa, pã, clause marker}, {ʃa, postposition, ʃa, ʃã, NA}, {ta, stem, ta, tã, modifier}, {tei, stem, tei, tẽĩ, modifier}, {ʋa, stem, ʋa, ʋ̃ã, nominalizer}, {ʋo, stem, ʋo, ʋ̃õ, conjunction}, {xa, stem, xa, xã, coordinator}, {pikó, stem, pikó, {pi, pĩ}, clause marker}, {rexé, postposition, rexé, {re, r̃ẽ}, NA}, {riré, postposition, riré, {ri, r̃ĩ}, conjunction} '''\n",
    "#split_morph_ids(tsvlang['synthesis']['morph_ids'])\n",
    "print(split_morph_ids(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a90ad26-f17e-457b-b1c6-03524e021bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just a debugging cell\n",
    "\n",
    "samplestrs = [\n",
    "#    'p, p',\n",
    "#    'e, {e, ɛ}, @, ɛ-lowering',\n",
    "#    's, {s, tʃ, ʃ}, {_i, i_}, {tʃ-fortition, tʃ-palatalization, ʃ-palatalization}',\n",
    "#    't, n, _+Ṽ, XMP=LN:t',\n",
    "#    \"e, {eː, ɛː}, $'_, {eː-lengthening, ɛː-lowering, ɛː-lengthening}\",\n",
    "#    'o, {o,ŏ}, _{h, ɾ}o, ŏ-vowel shortening',\n",
    "    \"o, ɑ, {$_$'{ʔɔ, hɔ},w_}, lowering\",\n",
    "    'a, {ə̃, ã}, {_{N, ND}, N_}, {ə̃-LN:V, ã-LN:V}'\n",
    "]\n",
    "for s in samplestrs:\n",
    "    pset = spreadsheet.split_outside_delims(s)\n",
    "    if len(pset) == 4:\n",
    "        print(pset)\n",
    "        phoneme, allos, env, proc = pset\n",
    "        mapping = alloprocs(allos, proc, phoneme)\n",
    "        print(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a67c36",
   "metadata": {},
   "source": [
    "## Read `.tsv` and v1 `.yaml` files\n",
    "\n",
    "Download, read, and process lang tabs (and existing v. 1 yaml files, if they exist). Set one or more tab indexes in `rng` to be checked for errors. Set `use_cached` to `True` if you want to use a previously-downloaded `.tsv` file instead of downloading from the input spreadsheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d89be0-d28b-447c-a4cf-6601ff31692b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nosynthesis = [\n",
    "    12,\n",
    "    21,\n",
    "    25,\n",
    "    31,\n",
    "    34,\n",
    "    36,\n",
    "    42,\n",
    "    43,\n",
    "    46,\n",
    "    47,\n",
    "    49, # Siriono\n",
    "    50,\n",
    "    53, # Tapirapé\n",
    "    56,\n",
    "    62,\n",
    "    64,\n",
    "    66\n",
    "]\n",
    "# investigate\n",
    "# 16 phonemes/allophones phonlist()\n",
    "# 21, 28, 34, 36, 42, 44, 51, 62, 64 proc details\n",
    "# 66 bad parse at high level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a2eb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cached = False\n",
    "langs = {}\n",
    "#rng = [0, 2, 3, 5, 20, 29, 37, 40] # Indexes that have no errors so far\n",
    "rng =  [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9]\n",
    "rng += [    11,                     17, 18, 19]  # List of one or more tab indexes to process\n",
    "rng += [20,         23, 24,         27,     29]\n",
    "rng += [30,     32, 33,                 38    ]\n",
    "rng += [40, 41,                         48, 49]\n",
    "rng += [            53,     55,         58, 59]\n",
    "rng += [60, 61,     63,               ] # 67 is out of bounds\n",
    "rng = [n for n in range(67)] # if n not in rng]\n",
    "rng = [e for e in rng if e not in nosynthesis]\n",
    "print(f'Working on {len(rng)} lang tabs')\n",
    "for row in ssdf.iloc[rng].itertuples():\n",
    "    if (yamldir / row.yaml).exists():\n",
    "        with open(yamldir / row.yaml, 'r', encoding='utf-8') as fh:\n",
    "            v1docs = list(yaml.safe_load_all(fh))\n",
    "            if math.isnan(v1docs[0]['coordinates'][0]['elevation_meters']):\n",
    "                v1docs[0]['coordinates'][0]['elevation_meters'] = None\n",
    "        v1synth = yaml2newyaml(v1docs[0])\n",
    "    else:\n",
    "        v1synth = {}\n",
    "    \n",
    "    tsvfile = langdir / f'{row.short}.tsv'\n",
    "    if use_cached is not True or not tsvfile.exists():\n",
    "        print(f\"Requesting '{row.tabname}' lang tab from index {row.Index} and caching at {tsvfile}.\")\n",
    "        r = requests.get(f'{spreadsheet.puburl}/pub?gid={row.gid}&single=true&output=tsv')\n",
    "        r.encoding = 'utf-8'\n",
    "        with open(tsvfile, 'w', encoding='utf-8') as out:\n",
    "            # Replace Windows CRLF with Unix LF\n",
    "            text = r.content.replace(b'\\r\\n', b'\\n').decode('utf8')\n",
    "            out.write(text)\n",
    "    else:\n",
    "        print(f\"Reading from cached file {tsvfile} ({row.Index}).\")\n",
    "    try:\n",
    "        v2synth, tsvlang, allophones = tsv2newyaml(tsvfile, v1synth)\n",
    "    except Exception as e:\n",
    "        v2synth = {}\n",
    "        sys.stderr.write(f'ERROR: spreadsheet tab {row.tabname} failed.\\n{e}\\n\\n')\n",
    "        raise e\n",
    "    langs[row.short] = {\n",
    "        'v1synth': v1synth,\n",
    "        'v2synth': v2synth,\n",
    "        'tsv': tsvlang['synthesis']\n",
    "    }\n",
    "    try:\n",
    "        with open(langdir / 'json' / f'{row.short}.json', 'w', encoding='utf8') as out:\n",
    "            json.dump([langs[row.short]['v2synth']], out, indent=2, ensure_ascii=False)\n",
    "        print(f'Dumped json {row.short}.json')\n",
    "    except Exception as e:\n",
    "        sys.stderr.write(f'Failed to dump json {row.short}.json.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d96c5f-d49a-45ef-bdb8-cb97655bd8e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
