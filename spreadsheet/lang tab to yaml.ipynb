{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d0a35eb",
   "metadata": {},
   "source": [
    "# Process a language tab to yaml v. 2.0\n",
    "\n",
    "This notebook collects language tabs from the SAPhon [Tupian Nasal Typology Input](https://docs.google.com/spreadsheets/d/1dvXFvLIV4y84CglgjAl-ZVb09IuGazs1SzFO_UJpmnI/edit#gid=1164878023) spreadsheet and creates version 2.0 yaml output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f1c03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spreadsheet\n",
    "import os, re, sys\n",
    "import unicodedata\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import json\n",
    "from jsonschema import validate, Draft202012Validator\n",
    "from jsonschema.exceptions import ValidationError, ErrorTree\n",
    "import math\n",
    "\n",
    "# From Michael Pollack\n",
    "import html_generator\n",
    "\n",
    "downloads = Path.home() / 'Downloads'\n",
    "langdir = Path('./newlangs/')\n",
    "(langdir / 'json').mkdir(parents=True, exist_ok=True)\n",
    "yamldir = Path('../langs')\n",
    "\n",
    "html_json_input_folder = langdir / 'json'\n",
    "synth_output_folder = Path('./en/synth_inv')\n",
    "ref_output_folder = Path('./en/ref_inv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f823926",
   "metadata": {},
   "source": [
    "## Get Tupian input spreadsheet lang tabs\n",
    "\n",
    "Collect the language tabs from the input spreadsheet into a dataframe, one row per lang tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ffb1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssdf = pd.DataFrame.from_records(list(spreadsheet.langsheets.values()))\n",
    "ssdf['tabname'] = list(spreadsheet.langsheets.keys())\n",
    "ssdf['yaml'] = ssdf['short'] + '.yaml'\n",
    "ssdf = ssdf[ssdf['include']].reset_index(drop=True).drop('include', axis='columns')\n",
    "assert(~ssdf['gid'].duplicated().any())\n",
    "assert(~ssdf['tabname'].duplicated().any())\n",
    "ssdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe35a452",
   "metadata": {},
   "source": [
    "## Download `.tsv` files (optional)\n",
    "\n",
    "The next cell is optional to download all lang tabs from the spreadsheet. Set `do_download` to `True` and execute the cell to do this task. For active work on a lang tab this step is not necessary and time-consuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fb2606",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_download = False\n",
    "if do_download:\n",
    "    for row in ssdf[ssdf['include']].itertuples():\n",
    "        r = requests.get(f'{spreadsheet.puburl}/pub?gid={row.gid}&single=true&output=tsv')\n",
    "        r.encoding = 'utf-8'\n",
    "        with open(langdir / f'{row.short}.tsv', 'w', encoding='utf-8') as out:\n",
    "            out.write(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd7ffc4",
   "metadata": {},
   "source": [
    "## Function definitions\n",
    "\n",
    "Functions used to create version 2.0 yaml, work in progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901c1421-f060-4922-9ff0-13ee9316b6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#langre = re.compile(\n",
    "#    r'''\n",
    "#    (?P<name>[^\\[]+)\n",
    "#    (?P<iso>\\[[^\\]]+\\])?\n",
    "#    ''',\n",
    "#    re.VERBOSE\n",
    "#)\n",
    "\n",
    "def _clean(s):\n",
    "    '''\n",
    "    Clean string of extraneous markup.\n",
    "    '''\n",
    "    if s != None:\n",
    "        s = s.strip().strip('{').strip('}').strip('[').strip(']').strip()\n",
    "    return s\n",
    "\n",
    "def split_morph_ids(mids):\n",
    "    morphs = []\n",
    "    if mids is None or mids == '' or mids.lower().strip() == 'none':\n",
    "        return []\n",
    "    for mid in spreadsheet.parse_with_delims(mids):\n",
    "        try:\n",
    "            d = {\n",
    "                'morpheme_id': mid[0],\n",
    "                'morpheme_type': mid[1],\n",
    "                'underlying_form': mid[2],\n",
    "#                'surface_forms': spreadsheet.parse_with_delims(mid[3].strip('{').strip('}')),\n",
    "                'surface_forms': [\n",
    "                    unicodedata.normalize('NFD', c.strip()) \\\n",
    "                        for c in mid[3].strip('{').strip('}').split(',')\n",
    "                ],\n",
    "                'gloss': mid[4]\n",
    "            }\n",
    "        except Exception as e:\n",
    "            msg = f'Could not parse morpheme ids: {mid}: {e}'\n",
    "            raise RuntimeError(msg)\n",
    "        morphs.append(d)\n",
    "    return morphs\n",
    "\n",
    "def alloprocs(allos, procs, phoneme):\n",
    "    '''\n",
    "    Return zipped allophones and processes extracted from allophone list.\n",
    "    '''\n",
    "    allolist = [_clean(a) for a in allos.split(',')]\n",
    "    proclist = [_clean(p) for p in procs.split(',')]\n",
    "    mapping = {a: [] for a in allolist}\n",
    "\n",
    "    try:\n",
    "        assert(len(proclist) > 0)\n",
    "    except AssertionError:\n",
    "        sys.stderr.write(f'Invalid entry for phoneme {phoneme}, must have at least one allophone and process\\n') \n",
    "    if (len(allolist) == 1):\n",
    "        mapping[allolist[0]] = proclist\n",
    "    else:\n",
    "        for proc in proclist:\n",
    "            m = spreadsheet.procre.match(proc)\n",
    "            pgd = m.groupdict()\n",
    "            try:\n",
    "                phone = pgd['phone'].replace('-', '')\n",
    "            except AttributeError:\n",
    "                sys.stderr.write(f'Process {proc} must have an allophone prefix\\n')\n",
    "                continue\n",
    "            procname = pgd['procsubtype'] if pgd['tag'] is None else pgd['tag'] + pgd['procsubtype']\n",
    "            try:\n",
    "                assert(phone in mapping.keys())\n",
    "                mapping[phone].append(procname)\n",
    "            except AssertionError:\n",
    "                sys.stderr.write(f'{proc} does not map to an allophone\\n')\n",
    "    for a, proclist in mapping.items():\n",
    "        if a == phoneme:\n",
    "            try:\n",
    "                assert(proclist == [])\n",
    "            except AssertionError:\n",
    "                if proclist == ['preservation']:\n",
    "                    pass\n",
    "                else:\n",
    "                    sys.stderr.write(f'Identity allophone {a} should not have a process\\n')\n",
    "        else:\n",
    "            try:\n",
    "                assert(len(proclist) > 0)\n",
    "            except AssertionError:\n",
    "                sys.stderr.write(f'Non-identity allophone {a} must name at least one process\\n')\n",
    "    return [{'allophone': k, 'processnames': v} for k, v in mapping.items()]\n",
    " \n",
    "\n",
    "def tsv2newyaml(tsvfile, v1):\n",
    "    '''\n",
    "    Make a new YAML dict from a Tupian input spreadsheet tab.\n",
    "    '''\n",
    "    tsvlang = spreadsheet.read_lang(tsvfile, strict=False)\n",
    "    natclasses, flatnatclasses, catsymb = spreadsheet.check_natclasses(tsvlang)\n",
    "    allophones, alloprocs = spreadsheet.check_allophones(tsvlang, flatnatclasses)\n",
    "    morph_id_map = spreadsheet.check_morpheme_ids(tsvlang)\n",
    "    spreadsheet.check_procs(tsvlang, flatnatclasses, morph_id_map, catsymb, alloprocs)\n",
    "    # TODO: remainder should be per-doc (synthesis, ref)\n",
    "    langdoc = {'info': {}, 'synthesis': {}, 'sources': []}\n",
    "    for doc in [tsvlang['synthesis']] + tsvlang['ref']:\n",
    "        is_synthesis = 'synthesis' in doc.keys()\n",
    "#        doc = tsvlang['synthesis']\n",
    "#        if 'lang' not in doc.keys():\n",
    "#            return (None, None, None)\n",
    "        try:\n",
    "            notes = doc['notes']\n",
    "        except KeyError:\n",
    "            notes = 'None'\n",
    "        if is_synthesis:\n",
    "            name = _clean(doc['lang'])\n",
    "            if '[' in name:\n",
    "                sys.stderr.write(f'Language name for {name} should be cleaned up.\\n')\n",
    "            try:\n",
    "                glottocode = _clean(doc['glottocode'])\n",
    "            except KeyError:\n",
    "                sys.stderr.write(f'Lang {name} missing glottocode field.\\n')\n",
    "                glottocode = TODO\n",
    "            try:\n",
    "                altnames = [_clean(c) for c in doc['altnames'].split(',')]\n",
    "            except KeyError:\n",
    "                sys.stderr.write(f'Lang {name} missing alternate_names field.\\n')\n",
    "                altnames = [TODO]\n",
    "            try:\n",
    "                iso_codes = [_clean(c) for c in doc['iso_codes'].split(',')]\n",
    "            except KeyError:\n",
    "                sys.stderr.write(f'Lang {name} missing iso_codes field.\\n')\n",
    "                iso_codes = [TODO]\n",
    "            if v1 == {}:\n",
    "                v1 = {\n",
    "                    'info': {\n",
    "                        'short_name': 'TODO',\n",
    "                        'alternate_names': 'TODO',\n",
    "                        'family': 'TODO',\n",
    "                        'countries': ['TODO'],\n",
    "                        'coordinates': [],\n",
    "                    },\n",
    "                    'synthesis': {\n",
    "                        'tone': False,\n",
    "                        'laryngeal_harmony': False\n",
    "                    }\n",
    "                }\n",
    "            try:\n",
    "                langdoc['info'] = {\n",
    "    #                'doctype': 'synthesis',\n",
    "                      'name': name,\n",
    "                      'short_name': v1['info']['short_name'],\n",
    "                      'alternate_names': altnames,\n",
    "                      'glottolog_code': glottocode,\n",
    "                      'iso_codes': iso_codes,\n",
    "                      'family': v1['info']['family'],\n",
    "                      'countries': v1['info']['countries'],\n",
    "                      'coordinates': v1['info']['coordinates'],\n",
    "                      'notes': 'None'\n",
    "                }\n",
    "                langdoc['synthesis'] = {\n",
    "                      'summary': doc['synthesis'],\n",
    "                      'notes': notes,\n",
    "                      'natural_classes': [{'symbol': nc[0], 'members': nc[1:]} for nc in natclasses['synthesis']],\n",
    "                      'phonemes': phonlist(allophones['synthesis']),\n",
    "                      'morphemes': split_morph_ids(doc['morph_ids']),\n",
    "                      'processdetails': doc['processes'],\n",
    "                 # TODO: following from v1 and not mentioned in new YAML draft\n",
    "    #!            'allophones': v1['allophones'],\n",
    "    #!            'nasal_harmony': v1['nasal_harmony'],\n",
    "                      'tone': v1['synthesis']['tone'],\n",
    "                      'laryngeal_harmony': v1['synthesis']['laryngeal_harmony'],\n",
    "                }\n",
    "            except Exception as e:\n",
    "                sys.stderr.write(f'Problem with doc {doc}.\\n')\n",
    "                raise e\n",
    "        else:\n",
    "            langdoc['sources'].append(\n",
    "                {\n",
    "                      'summary': doc['summary'],\n",
    "                      'notes': notes,\n",
    "                      'citation': [doc['source']],\n",
    "                      'natural_classes': [{'symbol': nc[0], 'members': nc[1:]} for nc in natclasses[doc['source']]],\n",
    "                      'phonemes': phonlist(allophones[doc['source']]),\n",
    "                      'morphemes': split_morph_ids(doc['morph_ids']),\n",
    "                      'processdetails': doc['processes'],\n",
    "                 # TODO: following from v1 and not mentioned in new YAML draft\n",
    "    #!            'allophones': v1['allophones'],\n",
    "    #!            'nasal_harmony': v1['nasal_harmony'],\n",
    "                }\n",
    "            )\n",
    "# Filter None values out of list values.\n",
    "#!    listflds = (\n",
    "#!        'alternate_names', 'iso_codes',\n",
    "#!        'countries', 'coordinates', 'natural_classes',\n",
    "#!        'morphemes', 'phonemes', 'processes',\n",
    "#!        'triggers', 'notes'\n",
    "#!    )\n",
    "#!    for fld in listflds:\n",
    "#!        sdoc[fld] = [v for v in sdoc[fld] if v is not None]\n",
    "    return (langdoc, tsvlang, allophones)\n",
    "\n",
    "def proclist_old(processes):\n",
    "    '''\n",
    "    Return a `processdetails` list from spreadsheet `processes` section.\n",
    "    '''\n",
    "    deets = []\n",
    "    for proc in processes:\n",
    "        procname = proc['proc_name'] if not '-' in proc['proc_name'] else proc['proc_name'][proc['proc_name'].index('-')+1:]\n",
    "        deet = {\n",
    "            'processname': procname,\n",
    "            'processtype': proc['proc_type'],\n",
    "            'description': proc['description'],\n",
    "            'optionality': proc['optionality'],\n",
    "            'directionality': proc['directionality'],\n",
    "            'alternation_type': proc['alternation_type']\n",
    "        }\n",
    "        for fld in 'undergoers', 'triggers':\n",
    "            if proc[fld] == 'NA':\n",
    "                deet[fld] = [['NA'], {'type': 'TODO', 'positional_restrictions': 'TODO'}]\n",
    "            elif isinstance(proc[fld], dict):\n",
    "                data = proc[fld][fld]\n",
    "            else:\n",
    "                print(f'PROC: {proc}')\n",
    "                data = proc[fld][0][fld]\n",
    "            try:\n",
    "                deet[fld] = [\n",
    "                    [_clean(f) for f in data.split(',')],\n",
    "                    {\n",
    "                        'type': 'TODO',\n",
    "                        'positional_restrictions': 'TODO'\n",
    "                    }\n",
    "                ]\n",
    "            except:\n",
    "                print(f'failed for {fld}: \"{proc[fld]}\"')\n",
    "        for new, old in ('transparent', 'transparencies'), ('opaque', 'opacities'):\n",
    "            deet[new] = proc[old][old]\n",
    "        deets.append(deet)\n",
    "    return deets\n",
    "\n",
    "def _clean_procname(p):\n",
    "    if p is None:\n",
    "        return 'TODO: got None'\n",
    "    else:\n",
    "        return _clean(p if '-' not in p else p[p.index('-')+1:])\n",
    "\n",
    "def phonlist(allophones):\n",
    "    '''\n",
    "    Return a `phonemes` list from an `allophone` set.\n",
    "    '''\n",
    "    phonemes = {}\n",
    "    for pset in allophones:\n",
    "        d = {}\n",
    "        if len(pset) == 5:\n",
    "            seg = pset[0]\n",
    "            envs = {'phoneme': f'STRING MAPPING: \"{pset}\"'}\n",
    "            print(f'STRING MAPPING: \"{pset}\"')\n",
    "            continue\n",
    "        elif len(pset) == 4:\n",
    "            phoneme, allos, _psetenv, proc = pset\n",
    "            phoneme = unicodedata.normalize('NFD', phoneme)\n",
    "            allophones = alloprocs(allos, proc, phoneme)\n",
    "            envs = []\n",
    "            for env in spreadsheet.parse_env(pset[2]):\n",
    "                try:\n",
    "                    env['allophones'] = allophones\n",
    "                except:\n",
    "                    print(env)\n",
    "                envs.append(env)\n",
    "        elif len(pset) == 2:\n",
    "            phoneme, allos = pset\n",
    "            phoneme = unicodedata.normalize('NFD', phoneme)\n",
    "            envs = [{\n",
    "                'preceding': '', # TODO: NA or other empty value here?\n",
    "                'following': '', # TODO: NA or other empty value here?\n",
    "                'allophones': [\n",
    "                    {\n",
    "                        'processnames': [],\n",
    "                        'allophone': unicodedata.normalize('NFD', _clean(a))\n",
    "                    } for a in allos.split(',')\n",
    "                ]\n",
    "            }]\n",
    "        else:\n",
    "            sys.stderr.write(f'Unexpected phoneme set length for \"{pset}\".')\n",
    "            continue\n",
    "        try:\n",
    "            phonemes[phoneme]['environments'] += envs\n",
    "        except KeyError:\n",
    "            phonemes[phoneme] = {'phoneme': phoneme, 'environments': envs}\n",
    "    # TODO: sort phonemes\n",
    "    return list(phonemes.values())\n",
    "\n",
    "def yaml2newyaml(v1):\n",
    "    '''\n",
    "    Copy a version 1 YAML dict into version 2.\n",
    "    '''\n",
    "    sdoc = {\n",
    "#        'doctype': 'synthesis',\n",
    "        'info': {\n",
    "          'name': v1['name'],\n",
    "          'short_name': v1['short_name'],\n",
    "          'alternate_names': v1['alternate_names'],\n",
    "#          'glottolog_code': v1['name'], # TODO: new, check by hand\n",
    "          'iso_codes': v1['iso_codes'],\n",
    "#          'glottolog_codes': [], # TODO: new, need to be added by hand\n",
    "          'family': v1['family'],\n",
    "          'countries': v1['countries'],\n",
    "          'coordinates': v1['coordinates'],\n",
    "          'notes': v1['notes'], # TODO: not mentioned in new YAML draft\n",
    "        },\n",
    "        'synthesis': {\n",
    "          'natural_classes': [], # TODO: new\n",
    "          'morphemes': [], # TODO: new?\n",
    "          'phonemes': [{'phoneme': p} for p in v1['phonemes']],\n",
    "          'processdetails': [], # TODO: new?\n",
    "          'triggers': [], # TODO: new?\n",
    "          'transparent': [], # TODO: new?, include?\n",
    "          'opaque': [], # TODO: new?, include?\n",
    "         # TODO: following from v1 and not mentioned in new YAML draft\n",
    "          'allophones': v1['allophones'],\n",
    "          'nasal_harmony': v1['nasal_harmony'],\n",
    "          'tone': v1['tone'],\n",
    "          'laryngeal_harmony': v1['laryngeal_harmony'],\n",
    "        }\n",
    "    }\n",
    "    # Filter None values out of list values.\n",
    "    infolistflds = (\n",
    "        'alternate_names', 'iso_codes', \n",
    "        'countries', 'coordinates', 'notes'\n",
    "    )\n",
    "    for fld in infolistflds:\n",
    "        sdoc['info'][fld] = [v for v in sdoc['info'][fld] if v is not None]\n",
    "    synthlistflds = ('natural_classes',\n",
    "        'morphemes', 'phonemes', 'processdetails',\n",
    "        'triggers'\n",
    "    )\n",
    "    for fld in synthlistflds:\n",
    "        sdoc['synthesis'][fld] = [v for v in sdoc['synthesis'][fld] if v is not None]\n",
    "    return sdoc\n",
    "\n",
    "TODO = 'TODO'\n",
    "\n",
    "def ss2refdoc(lang):\n",
    "    pass\n",
    "\n",
    "def ss2synthdoc(lang):\n",
    "    '''\n",
    "    Produce a synthesis yaml doc from a lang from the input spreadsheet.\n",
    "    '''\n",
    "    synth = lang['synthesis']\n",
    "    langm = re.match(langre, synth['lang'])\n",
    "    yd = {\n",
    "        'doctype': 'synthesis',\n",
    "        'name': langm['name'].strip(),\n",
    "        'short_name': TODO,\n",
    "        'alternate_names': TODO,\n",
    "        'iso_codes': langm['iso']\n",
    "    }\n",
    "    return yd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5391e535-ffd1-4c3f-8a7a-d51925623a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just a debugging cell\n",
    "\n",
    "sample = '''{i.3p, prefix, i, {j, ɲ}, 3rd person}, {pá, stem, pá, má, US}, {potá, stem, potá, motá, US}, {ɣʷá, postposition, ɣʷá, ŋʷá, US}, {ɣʷaré, postposition, ɣʷaré, ŋʷaré, US}, {tɨ́, suffix, tɨ́, nɨ́, US}, {pɨ́, suffix, pɨ́, mɨ́, US}, {kʷéra, stem, kʷéra, ŋʷéra, US}, {ɣʷivé, stem, ɣʷivé, ŋʷivé, US}, {pukú, stem, pukú, mukú, US}, {ɣʷasú, stem, ɣʷasú, ŋʷasú, US}, {kʷé, der.affix, kʷé, ŋʷé, US}, {pe, postposition, pe, {mẽ, ʋe, ʋ̃ẽ}, US}, {ʔó, der.affix, ʔó, ʔṍ, US}, {sé, der.affix, sé, sẽ́, US}, {ɣʷi, postposition, ɣʷi, ɣ̃ʷ̃ĩ, conjunction}, {ke, stem, ke, kẽ, modifier}, {ko, stem, ko, kõ, pronoun}, {ku, stem, ku, kũ, article}, {la, stem, la, l̃ã, article}, {na...i, stem, na...i, nã...ĩ, modifier}, {pa, stem, pa, pã, clause marker}, {ʃa, postposition, ʃa, ʃã, NA}, {ta, stem, ta, tã, modifier}, {tei, stem, tei, tẽĩ, modifier}, {ʋa, stem, ʋa, ʋ̃ã, nominalizer}, {ʋo, stem, ʋo, ʋ̃õ, conjunction}, {xa, stem, xa, xã, coordinator}, {pikó, stem, pikó, {pi, pĩ}, clause marker}, {rexé, postposition, rexé, {re, r̃ẽ}, NA}, {riré, postposition, riré, {ri, r̃ĩ}, conjunction} '''\n",
    "#split_morph_ids(tsvlang['synthesis']['morph_ids'])\n",
    "split_morph_ids(sample)\n",
    "#print([f'{t[\"surface_forms\"]}' for t in split_morph_ids(sample)])\n",
    "#spreadsheet.parse_with_delims('{j, ɲ}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a90ad26-f17e-457b-b1c6-03524e021bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just a debugging cell\n",
    "\n",
    "samplestrs = [\n",
    "#    'p, p',\n",
    "#    'e, {e, ɛ}, @, ɛ-lowering',\n",
    "#    's, {s, tʃ, ʃ}, {_i, i_}, {tʃ-fortition, tʃ-palatalization, ʃ-palatalization}',\n",
    "#    't, n, _+Ṽ, XMP=LN:t',\n",
    "#    \"e, {eː, ɛː}, $'_, {eː-lengthening, ɛː-lowering, ɛː-lengthening}\",\n",
    "#    'o, {o,ŏ}, _{h, ɾ}o, ŏ-vowel shortening',\n",
    "    \"o, ɑ, {$_$'{ʔɔ, hɔ},w_}, lowering\",\n",
    "    'a, {ə̃, ã}, {_{N, ND}, N_}, {ə̃-LN:V, ã-LN:V}'\n",
    "]\n",
    "for s in samplestrs:\n",
    "    pset = spreadsheet.split_outside_delims(s)\n",
    "    if len(pset) == 4:\n",
    "        print(pset)\n",
    "        phoneme, allos, env, proc = pset\n",
    "        mapping = alloprocs(allos, proc, phoneme)\n",
    "        print(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a67c36",
   "metadata": {},
   "source": [
    "## Read `.tsv` and v1 `.yaml` files\n",
    "\n",
    "Download, read, and process lang tabs (and existing v. 1 yaml files, if they exist). Set one or more tab indexes in `rng` to be checked for errors. Set `use_cached` to `True` if you want to use a previously-downloaded `.tsv` file instead of downloading from the input spreadsheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d874d0-c99a-4f8d-99ae-6caac634d254",
   "metadata": {},
   "outputs": [],
   "source": [
    "nosynthesis = [\n",
    "    12,\n",
    "    21,\n",
    "    25,\n",
    "    32,\n",
    "    35,\n",
    "    37,\n",
    "    43,\n",
    "    44,\n",
    "    47,\n",
    "    48,\n",
    "    50, # Siriono\n",
    "    51,\n",
    "    54, # Tapirapé\n",
    "    57,\n",
    "    63,\n",
    "    65,\n",
    "    67\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd4a19a-b7fd-43a1-9349-a0b6a36add32",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../saphonlang.schema.json', 'r', encoding='utf8') as sh:\n",
    "    schema = json.load(sh)\n",
    "validator = Draft202012Validator(schema)\n",
    "\n",
    "use_cached = True\n",
    "langs = {}\n",
    "rng = [n for n in range(0, 67) if n not in nosynthesis]\n",
    "print(f'Working on {len(rng)} lang tabs')\n",
    "myerror = None\n",
    "for row in ssdf.iloc[rng].itertuples():\n",
    "    if (yamldir / row.yaml).exists():\n",
    "        with open(yamldir / row.yaml, 'r', encoding='utf-8') as fh:\n",
    "            v1docs = list(yaml.safe_load_all(fh))\n",
    "            if math.isnan(v1docs[0]['coordinates'][0]['elevation_meters']):\n",
    "                v1docs[0]['coordinates'][0]['elevation_meters'] = 'Unspecified'\n",
    "        v1synth = yaml2newyaml(v1docs[0])\n",
    "    else:\n",
    "        v1synth = {}\n",
    "    \n",
    "    tsvfile = langdir / f'{row.short}.tsv'\n",
    "    if use_cached is not True or not tsvfile.exists():\n",
    "        print(f\"Requesting '{row.tabname}' lang tab from index {row.Index} and caching at {tsvfile}.\")\n",
    "        r = requests.get(f'{spreadsheet.puburl}/pub?gid={row.gid}&single=true&output=tsv')\n",
    "        r.encoding = 'utf-8'\n",
    "\n",
    "        with open(tsvfile, 'w', encoding='utf-8') as out:\n",
    "            # Replace Windows CRLF with Unix LF\n",
    "            text = r.content.replace(b'\\r\\n', b'\\n').decode('utf8')\n",
    "            out.write(text)\n",
    "    else:\n",
    "        print(f\"Reading from cached file {tsvfile} ({row.Index}).\")\n",
    "    try:\n",
    "        v2synth, tsvlang, allophones = tsv2newyaml(tsvfile, v1synth)\n",
    "        if tsvlang == None:\n",
    "            sys.stderr.write(f'Skipping .json creation for {row.tabname}.\\n\\n')\n",
    "            continue\n",
    "    except Exception as e:\n",
    "        v2synth = {}\n",
    "        print(f'\\n\\nERROR: spreadsheet tab {row.tabname} failed.\\n{e}\\n\\n')\n",
    "#        raise e\n",
    "        continue\n",
    "    langs[row.short] = {\n",
    "        'v1synth': v1synth,\n",
    "        'v2synth': v2synth,\n",
    "        'tsv': tsvlang['synthesis']\n",
    "    }\n",
    "    try:\n",
    "        langjson = langdir / 'json' / f'{row.short}.json'\n",
    "        with open(langjson, 'w', encoding='utf8') as out:\n",
    "            json.dump(langs[row.short]['v2synth'], out, indent=2, ensure_ascii=False)\n",
    "        print(f'Dumped json {row.short}.json')\n",
    "        \n",
    "    except Exception as e:\n",
    "        sys.stderr.write(f'Failed to dump json {row.short}.json.\\n')\n",
    "    try:\n",
    "        with open(langjson, 'r', encoding='utf8') as jh:\n",
    "            langobj = json.load(jh)\n",
    "#            tree = ErrorTree(validator.iter_errors(langobj))\n",
    "            validate(instance=langobj, schema=schema)\n",
    "    except ValidationError as e:\n",
    "        myerror = e\n",
    "        print(f'In json file {langjson}\\njson path {e.json_path}\\ngot value \"{e.instance}\"\\nexpected {e.validator_value}')\n",
    "#        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c437ab74-5cfc-48dc-a6fc-082d68b793ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the script to process all template files in the json folder and save the HTML files in the output folder\n",
    "print(f'Generating html files from json files in {langdir}')\n",
    "html_generator.process_templates_from_folder(\n",
    "    html_json_input_folder, synth_output_folder, ref_output_folder\n",
    ")\n",
    "print(f'Generated synthesis html files in {synth_output_folder}')\n",
    "print(f'Generated reference html files in {ref_output_folder}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4c0f09-efd7-4766-8bae-5874954e1454",
   "metadata": {},
   "source": [
    "### Extra stuff\n",
    "\n",
    "For debugging, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd4afbb-7190-41d2-b401-12e4bb1b6731",
   "metadata": {},
   "outputs": [],
   "source": [
    "langjson = langdir / 'json' / 'Ache.json'\n",
    "with open(langjson, 'r') as jh:\n",
    "    lang = json.load(jh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba613f0-e797-4c3f-a830-88ae52f08c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = lang['info']\n",
    "coordinates = info['coordinates']\n",
    "coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0563e0d-c55d-4702-8f09-1d50b8232485",
   "metadata": {},
   "outputs": [],
   "source": [
    "langjson, myerror.message, myerror.json_path, myerror.absolute_path, myerror.instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37825f20-e77b-47f4-bdb5-26912c5ad9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = Draft202012Validator(schema)\n",
    "for error in sorted(v.iter_errors(langobj), key=str):\n",
    "    print(error.message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
